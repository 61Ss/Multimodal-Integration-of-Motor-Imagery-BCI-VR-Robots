#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç²¾ç¡®äº’ä¿¡æ¯åˆ†æ - å…¨é‡280ä¸ªtrialsç‰ˆæœ¬

ğŸ¯ ç‰¹æ€§ï¼š
- ä½¿ç”¨çœŸå®çš„ä¿¡æ¯è®ºäº’ä¿¡æ¯è®¡ç®—
- æ”¯æŒè¿›åº¦ä¿å­˜å’Œæ¢å¤
- ä¼˜åŒ–çš„å¹¶è¡Œå¤„ç†
- è¯¦ç»†çš„æ—¶é—´ä¼°ç®—å’Œè¿›åº¦è·Ÿè¸ª
"""

import numpy as np
import pandas as pd
from scipy.signal import butter, sosfilt
from sklearn.feature_selection import mutual_info_regression
from sklearn.decomposition import IncrementalPCA
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestNeighbors
import time
import os
import sys
import pickle
import json
from functools import partial
from multiprocessing import Pool, cpu_count
from datetime import datetime, timedelta

# è§£å†³Windowsä¸ŠKMeansçš„å†…å­˜æ³„æ¼è­¦å‘Š
os.environ['OMP_NUM_THREADS'] = '2'

# å¯¼å…¥ç°æœ‰çš„æ•°æ®åŠ è½½å‡½æ•°
from visualize_eeg_psd import load_eeg_data

class FullExactMutualInformationAnalyzer:
    """
    å…¨é‡ç²¾ç¡®EEGäº’ä¿¡æ¯åˆ†æå™¨
    
    æ”¯æŒï¼š
    1. 280ä¸ªtrialsçš„å®Œæ•´åˆ†æ
    2. è¿›åº¦ä¿å­˜å’Œæ¢å¤
    3. æ™ºèƒ½å¹¶è¡Œå¤„ç†
    4. è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ª
    """
    
    def __init__(self, subject_id='aw', random_state=42, verbose=True, 
                 n_jobs=None, mi_neighbors=3, checkpoint_dir='checkpoints'):
        """
        åˆå§‹åŒ–å…¨é‡ç²¾ç¡®åˆ†æå™¨
        
        Parameters:
        -----------
        n_jobs : int or None
            å¹¶è¡Œè®¡ç®—çš„è¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        mi_neighbors : int
            äº’ä¿¡æ¯è®¡ç®—ä¸­çš„é‚»å±…æ•°ï¼ˆå½±å“ç²¾åº¦å’Œé€Ÿåº¦ï¼‰
        checkpoint_dir : str
            æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        """
        self.subject_id = subject_id
        self.random_state = random_state
        self.verbose = verbose
        self.n_jobs = n_jobs if n_jobs is not None else min(cpu_count(), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°
        self.mi_neighbors = mi_neighbors
        self.checkpoint_dir = checkpoint_dir
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # é¢‘æ®µå®šä¹‰
        self.bands = {
            'alpha': (7, 13),
            'beta': (14, 30), 
            'gamma': (30, 100)
        }
        
        # PCAå’Œèšç±»å‚æ•°
        self.pca_components = 20
        self.kmeans_clusters = 2
        self.n_neighbors = 5
        self.density_percentile = 85
        
        # æ—¶é—´è·Ÿè¸ª
        self.start_time = None
        self.trial_times = []
        
    def save_checkpoint(self, results, trial_idx, metadata=None):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_data = {
            'results': results,
            'completed_trials': trial_idx + 1,
            'total_trials': len(self.trials),
            'timestamp': datetime.now().isoformat(),
            'subject_id': self.subject_id,
            'metadata': metadata or {}
        }
        
        checkpoint_file = os.path.join(
            self.checkpoint_dir, 
            f'checkpoint_{self.subject_id}_trial_{trial_idx+1}.pkl'
        )
        
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(checkpoint_data, f)
    
    def load_checkpoint(self):
        """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
        checkpoint_files = []
        for file in os.listdir(self.checkpoint_dir):
            if file.startswith(f'checkpoint_{self.subject_id}_') and file.endswith('.pkl'):
                trial_num = int(file.split('_trial_')[1].split('.pkl')[0])
                checkpoint_files.append((trial_num, file))
        
        if not checkpoint_files:
            return None, 0
        
        # æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹
        latest_trial, latest_file = max(checkpoint_files)
        checkpoint_path = os.path.join(self.checkpoint_dir, latest_file)
        
        try:
            with open(checkpoint_path, 'rb') as f:
                checkpoint_data = pickle.load(f)
            
            if self.verbose:
                print(f"ğŸ“ æ¢å¤æ£€æŸ¥ç‚¹: {checkpoint_data['completed_trials']}/{checkpoint_data['total_trials']} trials")
            
            return checkpoint_data, latest_trial
        
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None, 0
    
    def estimate_remaining_time(self, completed_trials, total_trials):
        """ä¼°ç®—å‰©ä½™æ—¶é—´"""
        if len(self.trial_times) < 2:
            return "è®¡ç®—ä¸­..."
        
        avg_time_per_trial = np.mean(self.trial_times)
        remaining_trials = total_trials - completed_trials
        remaining_seconds = avg_time_per_trial * remaining_trials
        
        if remaining_seconds < 60:
            return f"{remaining_seconds:.0f}ç§’"
        elif remaining_seconds < 3600:
            return f"{remaining_seconds/60:.1f}åˆ†é’Ÿ"
        else:
            hours = remaining_seconds // 3600
            minutes = (remaining_seconds % 3600) // 60
            return f"{hours:.0f}å°æ—¶{minutes:.0f}åˆ†é’Ÿ"
    
    def calculate_exact_mutual_information_matrix(self, data, band_name=""):
        """è®¡ç®—ç²¾ç¡®çš„äº’ä¿¡æ¯çŸ©é˜µ"""
        num_channels = data.shape[1]
        mi_matrix = np.zeros((num_channels, num_channels))
        
        # è®¡ç®—æ‰€æœ‰é€šé“å¯¹çš„äº’ä¿¡æ¯
        channel_pairs = [(i, j) for i in range(num_channels) for j in range(i + 1, num_channels)]
        total_pairs = len(channel_pairs)
        
        start_time = time.time()
        
        # å¹¶è¡Œè®¡ç®—
        mi_func = partial(self._compute_exact_mi_pair, data=data)
        with Pool(self.n_jobs) as pool:
            mi_values = pool.map(mi_func, channel_pairs)
        
        calc_time = time.time() - start_time
        
        # å¡«å……å¯¹ç§°çŸ©é˜µ
        for (i, j), mi_value in zip(channel_pairs, mi_values):
            mi_matrix[i, j] = mi_value
            mi_matrix[j, i] = mi_value
        
        # å¯¹è§’çº¿è®¾ä¸º0
        np.fill_diagonal(mi_matrix, 0)
        
        return mi_matrix
    
    def _compute_exact_mi_pair(self, channel_pair, data):
        """è®¡ç®—å•ä¸ªé€šé“å¯¹çš„ç²¾ç¡®äº’ä¿¡æ¯ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰"""
        i, j = channel_pair
        mi_value = mutual_info_regression(
            data[:, i].reshape(-1, 1), 
            data[:, j],
            discrete_features=False,
            n_neighbors=self.mi_neighbors,
            random_state=self.random_state
        )[0]
        return mi_value
    
    def apply_bandpass_filter(self, data, band, fs):
        """åº”ç”¨å¸¦é€šæ»¤æ³¢"""
        fmin, fmax = band
        nyquist = fs / 2
        low = fmin / nyquist
        high = min(fmax / nyquist, 0.99)
        
        # ä½¿ç”¨SOSæ ¼å¼æ»¤æ³¢å™¨
        sos = butter(2, [low, high], btype='band', output='sos')
        
        filtered_data = np.zeros_like(data)
        for ch in range(data.shape[1]):
            filtered_data[:, ch] = sosfilt(sos, data[:, ch])
        
        return filtered_data
    
    def analyze_single_trial_exact(self, trial_idx):
        """ç²¾ç¡®åˆ†æå•ä¸ªtrial"""
        trial_start_time = time.time()
        
        trial_data = self.trials[trial_idx]
        
        # å­˜å‚¨å„é¢‘æ®µçš„äº’ä¿¡æ¯çŸ©é˜µ
        band_mi_matrices = {}
        
        for band_name, band_range in self.bands.items():
            # æ»¤æ³¢
            filtered_data = self.apply_bandpass_filter(trial_data, band_range, self.fs)
            
            # è®¡ç®—ç²¾ç¡®äº’ä¿¡æ¯çŸ©é˜µ
            mi_matrix = self.calculate_exact_mutual_information_matrix(filtered_data, band_name)
            band_mi_matrices[band_name] = mi_matrix
        
        trial_time = time.time() - trial_start_time
        self.trial_times.append(trial_time)
        
        return band_mi_matrices
    
    def load_and_preprocess_data(self):
        """åŠ è½½å¹¶é¢„å¤„ç†EEGæ•°æ®"""
        if self.verbose:
            print("ğŸ”„ åŠ è½½æ•°æ®...")
        
        trials, fs, ch_names, ch_pos, trial_labels, regions = load_eeg_data(self.subject_id)
        
        self.trials = trials
        self.fs = fs
        self.ch_names = ch_names
        self.ch_pos = ch_pos
        self.trial_labels = trial_labels
        self.regions = regions
        
        if self.verbose:
            print(f"âœ“ æ•°æ®: {trials.shape}, è¿›ç¨‹: {self.n_jobs}")
        
        return trials
    
    def calculate_density_labels_fast(self):
        """å¿«é€Ÿè®¡ç®—å¯†åº¦æ ‡ç­¾"""
        if self.verbose:
            print("ğŸ”„ è®¡ç®—å¯†åº¦æ ‡ç­¾...")
        
        num_trials, num_timepoints, num_channels = self.trials.shape
        
        # å¤§å¹…é‡‡æ ·ä»¥åŠ é€ŸPCA
        sample_step = max(1, num_timepoints // 50)
        sampled_trials = self.trials[:, ::sample_step, :]
        flattened_trials = sampled_trials.reshape(num_trials, -1)
        
        # å¿«é€ŸPCA
        pca = IncrementalPCA(n_components=self.pca_components, batch_size=min(50, num_trials))
        
        batch_size = min(50, num_trials)
        for i in range(0, num_trials, batch_size):
            batch = flattened_trials[i:i + batch_size]
            pca.partial_fit(batch)
        
        pca_results = pca.transform(flattened_trials)
        
        # K-meansèšç±»
        kmeans = KMeans(n_clusters=self.kmeans_clusters, random_state=self.random_state, n_init=3)
        cluster_labels = kmeans.fit_predict(pca_results)
        
        # å¯†åº¦è®¡ç®—
        nn = NearestNeighbors(n_neighbors=min(self.n_neighbors, num_trials//2))
        nn.fit(pca_results)
        distances, _ = nn.kneighbors(pca_results)
        density = np.mean(distances, axis=1)
        
        # å¯†åº¦æ ‡ç­¾åˆ†é…
        clusters = np.unique(cluster_labels)
        density_labels = [''] * num_trials
        
        for cluster in clusters:
            cluster_mask = cluster_labels == cluster
            threshold = np.percentile(density[cluster_mask], self.density_percentile)
            
            for i in range(num_trials):
                if cluster_mask[i]:
                    density_labels[i] = 'high' if density[i] <= threshold else 'low'
        
        density_labels = np.array(density_labels, dtype='U10')
        self.density_labels = density_labels
        self.cluster_labels = cluster_labels
        
        if self.verbose:
            high_count = np.sum(density_labels == 'high')
            low_count = np.sum(density_labels == 'low')
            print(f"âœ“ å¯†åº¦æ ‡ç­¾: High={high_count}, Low={low_count}")
        
        return density_labels
    
    def calculate_average_mi_and_rank_channels(self, band_mi_matrices):
        """è®¡ç®—å¹³å‡äº’ä¿¡æ¯å¹¶æ’åºé€šé“"""
        # è®¡ç®—ä¸‰ä¸ªé¢‘æ®µçš„å¹³å‡
        alpha_mi = band_mi_matrices['alpha']
        beta_mi = band_mi_matrices['beta']
        gamma_mi = band_mi_matrices['gamma']
        
        avg_mi_matrix = (alpha_mi + beta_mi + gamma_mi) / 3
        
        # è®¡ç®—æ¯ä¸ªé€šé“çš„è¿æ¥å¼ºåº¦
        channel_connectivity = np.sum(avg_mi_matrix, axis=1)
        
        # æ’åºè·å–å‰4ä¸ªé€šé“
        top_channels_indices = np.argsort(channel_connectivity)[-4:][::-1]
        top_channels_scores = channel_connectivity[top_channels_indices]
        top_channels_names = [self.ch_names[i] for i in top_channels_indices]
        
        return avg_mi_matrix, top_channels_indices, top_channels_scores, top_channels_names
    
    def run_full_exact_analysis(self, resume=True):
        """è¿è¡Œå®Œæ•´çš„ç²¾ç¡®åˆ†æ"""
        if self.verbose:
            print("ğŸ¯ å…¨é‡ç²¾ç¡®EEGäº’ä¿¡æ¯åˆ†æ - 280ä¸ªtrials")
        
        self.start_time = time.time()
        
        # åŠ è½½æ•°æ®
        self.load_and_preprocess_data()
        
        # è®¡ç®—å¯†åº¦æ ‡ç­¾
        self.calculate_density_labels_fast()
        
        num_trials = self.trials.shape[0]
        results = []
        start_trial = 0
        
        # å°è¯•æ¢å¤æ£€æŸ¥ç‚¹
        if resume:
            checkpoint_data, last_completed = self.load_checkpoint()
            if checkpoint_data:
                results = checkpoint_data['results']
                start_trial = checkpoint_data['completed_trials']
                
                # æ¢å¤ä¿¡æ¯å·²åœ¨load_checkpointä¸­æ˜¾ç¤º
                
                if start_trial >= num_trials:
                    if self.verbose:
                        print("âœ… æ‰€æœ‰trialså·²å®Œæˆ!")
                    return results, None
        
        if self.verbose:
            estimated_total_time = (num_trials - start_trial) * 60  # å‡è®¾æ¯trial 60ç§’
            print(f"ğŸš€ å¼€å§‹åˆ†æ: {start_trial}->{num_trials-1} (é¢„è®¡{estimated_total_time/3600:.1f}h)")
        
        # åˆ†æå‰©ä½™çš„trials
        for trial_idx in range(start_trial, num_trials):
            trial_overall_start = time.time()
            
            # åˆ†æå½“å‰trial
            band_mi_matrices = self.analyze_single_trial_exact(trial_idx)
            avg_mi_matrix, top_indices, top_scores, top_names = self.calculate_average_mi_and_rank_channels(band_mi_matrices)
            
            # ä¿å­˜ç»“æœ
            trial_result = {
                'trial_idx': trial_idx,
                'original_label': self.trial_labels[trial_idx],
                'density_label': self.density_labels[trial_idx],
                'cluster_label': self.cluster_labels[trial_idx],
                'top_4_channels': top_names,
                'top_4_indices': top_indices,
                'top_4_scores': top_scores,
            }
            
            results.append(trial_result)
            
            trial_total_time = time.time() - trial_overall_start
            
            # è¿›åº¦æ˜¾ç¤º - åªåœ¨å…³é”®èŠ‚ç‚¹æ˜¾ç¤º
            completed = trial_idx - start_trial + 1
            total_remaining = num_trials - start_trial
            progress_pct = 100 * completed / total_remaining
            
            # æ¯10ä¸ªtrialsä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹å¹¶æ˜¾ç¤ºè¿›åº¦
            if (trial_idx + 1) % 10 == 0:
                self.save_checkpoint(results, trial_idx)
                if self.verbose and len(self.trial_times) > 0:
                    avg_time = np.mean(self.trial_times)
                    remaining_time_str = self.estimate_remaining_time(trial_idx + 1, num_trials)
                    print(f"âœ“ {trial_idx+1}/{num_trials} ({progress_pct:.0f}%) - {avg_time:.0f}s/trial - å‰©ä½™{remaining_time_str}")
            elif (trial_idx + 1) % 5 == 0:
                self.save_checkpoint(results, trial_idx)
        
        # æœ€ç»ˆæ£€æŸ¥ç‚¹
        if results:
            self.save_checkpoint(results, num_trials - 1)
        
        total_time = time.time() - self.start_time
        
        if self.verbose:
            print(f"\nğŸ‰ åˆ†æå®Œæˆ! ç”¨æ—¶{total_time/3600:.1f}h, å¹³å‡{total_time/num_trials:.0f}s/trial")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_csv = f'exact_full_mi_results_{self.subject_id}.csv'
        data_rows = []
        for result in results:
            row = {
                'trial_idx': result['trial_idx'],
                'original_label': result['original_label'],
                'density_label': result['density_label'],
                'cluster_label': result['cluster_label'],
                'top_channel_1': result['top_4_channels'][0],
                'top_channel_1_score': result['top_4_scores'][0],
                'top_channel_2': result['top_4_channels'][1],
                'top_channel_2_score': result['top_4_scores'][1],
                'top_channel_3': result['top_4_channels'][2],
                'top_channel_3_score': result['top_4_scores'][2],
                'top_channel_4': result['top_4_channels'][3],
                'top_channel_4_score': result['top_4_scores'][3],
            }
            data_rows.append(row)
        
        df = pd.DataFrame(data_rows)
        df.to_csv(output_csv, index=False, encoding='utf-8-sig')
        
        if self.verbose:
            print(f"âœ“ ç»“æœä¿å­˜: {output_csv}")
        
        return results, df

def main():
    """ä¸»å‡½æ•°"""
    if '--help' in sys.argv:
        print("å…¨é‡ç²¾ç¡®äº’ä¿¡æ¯åˆ†æ - 280ä¸ªtrials")
        print("ç”¨æ³•: python exact_full_analysis.py [é€‰é¡¹]")
        print("é€‰é¡¹:")
        print("  --no-resume    : ä¸æ¢å¤æ£€æŸ¥ç‚¹ï¼Œé‡æ–°å¼€å§‹")
        print("  --jobs=N       : è®¾ç½®å¹¶è¡Œè¿›ç¨‹æ•°")
        print("  --help         : æ˜¾ç¤ºå¸®åŠ©")
        print("\nâš ï¸ è­¦å‘Šï¼šé¢„è®¡éœ€è¦4-6å°æ—¶å®Œæˆå…¨éƒ¨280ä¸ªtrials")
        return
    
    # è§£æå‚æ•°
    resume = '--no-resume' not in sys.argv
    n_jobs = None
    
    for arg in sys.argv:
        if arg.startswith('--jobs='):
            try:
                n_jobs = int(arg.split('=')[1])
            except ValueError:
                print("âš ï¸ æ— æ•ˆçš„jobså‚æ•°")
    
    # æ—¶é—´è­¦å‘Š
    print("ğŸš€ å…¨é‡ç²¾ç¡®EEGäº’ä¿¡æ¯åˆ†æ")
    print("âš ï¸ é¢„è®¡è€—æ—¶4-6å°æ—¶ï¼Œä½¿ç”¨çœŸå®äº’ä¿¡æ¯è®¡ç®—")
    resume_msg = "æ¢å¤è¿›åº¦" if resume else "é‡æ–°å¼€å§‹"
    print(f"ğŸ“‹ {resume_msg}, è¿›ç¨‹æ•°: {n_jobs or 'è‡ªåŠ¨'}, æ”¯æŒä¸­æ–­æ¢å¤")
    
    response = input("ç¡®è®¤å¼€å§‹å…¨é‡åˆ†æå—ï¼Ÿ (y/N): ")
    if response.lower() != 'y':
        print("å·²å–æ¶ˆ")
        return
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = FullExactMutualInformationAnalyzer(
        subject_id='aw',
        random_state=42,
        verbose=True,
        n_jobs=n_jobs,
        mi_neighbors=3
    )
    
    # è¿è¡Œåˆ†æ
    try:
        results, df = analyzer.run_full_exact_analysis(resume=resume)
        print(f"\nğŸ‰ å…¨é‡åˆ†ææˆåŠŸå®Œæˆ! å…±å¤„ç† {len(results)} ä¸ªtrials")
    except KeyboardInterrupt:
        print(f"\nâ¸ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        print("è¿›åº¦å·²ä¿å­˜ï¼Œå¯ä»¥ç¨åä½¿ç”¨ç›¸åŒå‘½ä»¤æ¢å¤")
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()