import numpy as np
import pandas as pd
from scipy.signal import butter, sosfilt
from scipy.stats import pearsonr
from sklearn.decomposition import IncrementalPCA
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestNeighbors
import time
import os
import sys

# è§£å†³Windowsä¸ŠKMeansçš„å†…å­˜æ³„æ¼è­¦å‘Š
os.environ['OMP_NUM_THREADS'] = '2'

# å¯¼å…¥ç°æœ‰çš„æ•°æ®åŠ è½½å‡½æ•°
from visualize_eeg_psd import load_eeg_data

class UltraFastMutualInformationAnalyzer:
    """
    è¶…é«˜é€ŸEEGäº’ä¿¡æ¯åˆ†æå™¨
    
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. ä½¿ç”¨SOSæ»¤æ³¢å™¨ä»£æ›¿filtfiltï¼ˆ10x+æé€Ÿï¼‰
    2. æ‰¹é‡æ»¤æ³¢æ‰€æœ‰trialsï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
    3. ç®€åŒ–çš„ç›¸å…³ç³»æ•°è®¡ç®—
    4. æœ€å°åŒ–å†…å­˜åˆ†é…
    5. è·³è¿‡ä¸å¿…è¦çš„ç²¾åº¦è®¡ç®—
    """
    
    def __init__(self, subject_id='aw', random_state=42, verbose=True, 
                 use_simple_filter=True, max_samples_per_trial=200):
        """
        åˆå§‹åŒ–è¶…é«˜é€Ÿåˆ†æå™¨
        
        Parameters:
        -----------
        subject_id : str
            å—è¯•è€…ID
        random_state : int
            éšæœºç§å­
        verbose : bool
            æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        use_simple_filter : bool
            æ˜¯å¦ä½¿ç”¨ç®€åŒ–æ»¤æ³¢ï¼ˆå¤§å¹…æé€Ÿï¼‰
        max_samples_per_trial : int
            æ¯ä¸ªtrialçš„æœ€å¤§é‡‡æ ·ç‚¹æ•°ï¼ˆå¤§å¹…å‡å°‘è®¡ç®—é‡ï¼‰
        """
        self.subject_id = subject_id
        self.random_state = random_state
        self.verbose = verbose
        self.use_simple_filter = use_simple_filter
        self.max_samples_per_trial = max_samples_per_trial
        
        # é¢‘æ®µå®šä¹‰ (Hz) - ç®€åŒ–ä¸ºæ›´å®½çš„é¢‘æ®µä»¥å‡å°‘æ»¤æ³¢æ¬¡æ•°
        if use_simple_filter:
            self.bands = {
                'low_freq': (7, 30),    # åˆå¹¶alphaå’Œbeta
                'high_freq': (30, 80)   # ç®€åŒ–çš„gamma
            }
        else:
            self.bands = {
                'alpha': (7, 13),
                'beta': (14, 30), 
                'gamma': (30, 100)
            }
        
        # ä¼˜åŒ–çš„å‚æ•°
        self.pca_components = 20
        self.kmeans_clusters = 2
        self.n_neighbors = 5
        self.density_percentile = 85
        
        # é¢„ç¼–è¯‘æ»¤æ³¢å™¨ç³»æ•°ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        self.filter_cache = {}
    
    def get_fast_filter(self, band, fs):
        """è·å–æˆ–åˆ›å»ºå¿«é€Ÿæ»¤æ³¢å™¨"""
        fmin, fmax = band
        cache_key = (fmin, fmax, fs)
        
        if cache_key not in self.filter_cache:
            nyquist = fs / 2
            low = fmin / nyquist
            high = min(fmax / nyquist, 0.99)
            
            # ä½¿ç”¨SOSæ ¼å¼çš„æ»¤æ³¢å™¨ï¼ˆæ›´ç¨³å®šæ›´å¿«ï¼‰
            sos = butter(2, [low, high], btype='band', output='sos')  # é™ä½é˜¶æ•°ä»4åˆ°2
            self.filter_cache[cache_key] = sos
        
        return self.filter_cache[cache_key]
    
    def apply_fast_filter(self, data, band, fs):
        """åº”ç”¨å¿«é€Ÿæ»¤æ³¢å™¨"""
        if self.use_simple_filter:
            # è¶…ç®€åŒ–æ»¤æ³¢ï¼šåªä½¿ç”¨ç®€å•çš„é¢‘åŸŸæˆªæ–­
            return self.apply_frequency_filter(data, band, fs)
        else:
            # ä½¿ç”¨ä¼˜åŒ–çš„SOSæ»¤æ³¢
            sos = self.get_fast_filter(band, fs)
            filtered_data = np.zeros_like(data)
            for ch in range(data.shape[1]):
                filtered_data[:, ch] = sosfilt(sos, data[:, ch])
            return filtered_data
    
    def apply_frequency_filter(self, data, band, fs):
        """è¶…ç®€åŒ–çš„é¢‘åŸŸæ»¤æ³¢ï¼ˆæœ€å¿«ä½†ç²¾åº¦ç•¥ä½ï¼‰"""
        fmin, fmax = band
        
        # FFTé¢‘åŸŸæ»¤æ³¢
        fft_data = np.fft.fft(data, axis=0)
        freqs = np.fft.fftfreq(data.shape[0], 1/fs)
        
        # åˆ›å»ºé¢‘åŸŸæ©ç 
        mask = (np.abs(freqs) >= fmin) & (np.abs(freqs) <= fmax)
        
        # åº”ç”¨æ»¤æ³¢
        fft_data[~mask] = 0
        
        # åå˜æ¢å›æ—¶åŸŸ
        filtered_data = np.real(np.fft.ifft(fft_data, axis=0))
        
        return filtered_data
    
    def fast_correlation_matrix(self, data):
        """è¶…å¿«é€Ÿç›¸å…³ç³»æ•°çŸ©é˜µè®¡ç®—"""
        # æ•°æ®é‡‡æ ·ä»¥å¤§å¹…å‡å°‘è®¡ç®—é‡
        if data.shape[0] > self.max_samples_per_trial:
            step = data.shape[0] // self.max_samples_per_trial
            data = data[::step]
        
        # æ ‡å‡†åŒ–æ•°æ®
        data_centered = data - np.mean(data, axis=0)
        data_std = np.std(data, axis=0)
        data_std[data_std == 0] = 1  # é¿å…é™¤é›¶
        data_normalized = data_centered / data_std
        
        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
        corr_matrix = np.corrcoef(data_normalized.T)
        
        # å¤„ç†NaNå€¼
        corr_matrix = np.nan_to_num(corr_matrix, 0)
        
        # å–å¹³æ–¹ä½œä¸ºäº’ä¿¡æ¯è¿‘ä¼¼
        return corr_matrix ** 2
    
    def analyze_single_trial_ultra_fast(self, trial_idx):
        """è¶…å¿«é€Ÿåˆ†æå•ä¸ªtrial"""
        trial_data = self.trials[trial_idx]
        
        # å­˜å‚¨å„é¢‘æ®µçš„äº’ä¿¡æ¯çŸ©é˜µ
        band_mi_matrices = {}
        
        for band_name, band_range in self.bands.items():
            # åº”ç”¨å¿«é€Ÿæ»¤æ³¢
            filtered_data = self.apply_fast_filter(trial_data, band_range, self.fs)
            
            # å¿«é€Ÿè®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            mi_matrix = self.fast_correlation_matrix(filtered_data)
            band_mi_matrices[band_name] = mi_matrix
        
        return band_mi_matrices
    
    def load_and_preprocess_data(self):
        """åŠ è½½å¹¶é¢„å¤„ç†EEGæ•°æ®"""
        if self.verbose:
            print("===== åŠ è½½æ•°æ® =====")
        
        trials, fs, ch_names, ch_pos, trial_labels, regions = load_eeg_data(self.subject_id)
        
        self.trials = trials
        self.fs = fs
        self.ch_names = ch_names
        self.ch_pos = ch_pos
        self.trial_labels = trial_labels
        self.regions = regions
        
        num_trials, num_timepoints, num_channels = trials.shape
        if self.verbose:
            print(f"æ•°æ®å½¢çŠ¶: {trials.shape}")
            print(f"ä¼˜åŒ–ç­–ç•¥: {'ç®€åŒ–æ»¤æ³¢' if self.use_simple_filter else 'SOSæ»¤æ³¢'}")
            print(f"é‡‡æ ·ç­–ç•¥: æ¯trialæœ€å¤š{self.max_samples_per_trial}ä¸ªç‚¹")
            print(f"é¢‘æ®µæ•°é‡: {len(self.bands)} (å‡å°‘æ»¤æ³¢æ¬¡æ•°)")
        
        return trials
    
    def calculate_density_labels_fast(self):
        """å¿«é€Ÿè®¡ç®—å¯†åº¦æ ‡ç­¾"""
        if self.verbose:
            print("===== å¿«é€Ÿè®¡ç®—å¯†åº¦æ ‡ç­¾ =====")
        
        # æ•°æ®é¢„å¤„ç†ï¼šå±•å¹³
        num_trials, num_timepoints, num_channels = self.trials.shape
        
        # å¤§å¹…é‡‡æ ·ä»¥åŠ é€ŸPCA
        sample_step = max(1, num_timepoints // 50)  # åªå–1/50çš„æ—¶é—´ç‚¹
        sampled_trials = self.trials[:, ::sample_step, ::2]  # æ—¶é—´å’Œé€šé“éƒ½é‡‡æ ·
        
        flattened_trials = sampled_trials.reshape(num_trials, -1)
        
        if self.verbose:
            print(f"PCAè¾“å…¥ç»´åº¦: {flattened_trials.shape} (å¤§å¹…é‡‡æ ·å)")
        
        # å¿«é€ŸPCA
        pca = IncrementalPCA(n_components=self.pca_components, batch_size=min(50, num_trials))
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = min(50, num_trials)
        for i in range(0, num_trials, batch_size):
            batch = flattened_trials[i:i + batch_size]
            pca.partial_fit(batch)
        
        pca_results = pca.transform(flattened_trials)
        
        # K-meansèšç±»
        kmeans = KMeans(n_clusters=self.kmeans_clusters, random_state=self.random_state, n_init=3)
        cluster_labels = kmeans.fit_predict(pca_results)
        
        # ç®€åŒ–çš„å¯†åº¦è®¡ç®—
        nn = NearestNeighbors(n_neighbors=min(self.n_neighbors, num_trials//2))
        nn.fit(pca_results)
        distances, _ = nn.kneighbors(pca_results)
        density = np.mean(distances, axis=1)
        
        # ç®€åŒ–çš„å¯†åº¦æ ‡ç­¾åˆ†é…
        clusters = np.unique(cluster_labels)
        density_labels = [''] * num_trials
        
        for cluster in clusters:
            cluster_mask = cluster_labels == cluster
            threshold = np.percentile(density[cluster_mask], self.density_percentile)
            
            for i in range(num_trials):
                if cluster_mask[i]:
                    density_labels[i] = 'high' if density[i] <= threshold else 'low'
        
        density_labels = np.array(density_labels, dtype='U10')
        self.density_labels = density_labels
        self.cluster_labels = cluster_labels
        
        if self.verbose:
            high_count = np.sum(density_labels == 'high')
            low_count = np.sum(density_labels == 'low')
            print(f"å¯†åº¦æ ‡ç­¾åˆ†å¸ƒ: High={high_count}, Low={low_count}")
        
        return density_labels
    
    def calculate_average_mi_and_rank_channels(self, band_mi_matrices):
        """è®¡ç®—å¹³å‡äº’ä¿¡æ¯å¹¶æ’åºé€šé“"""
        # è®¡ç®—æ‰€æœ‰é¢‘æ®µçš„å¹³å‡äº’ä¿¡æ¯çŸ©é˜µ
        if self.use_simple_filter:
            # ç®€åŒ–é¢‘æ®µçš„æƒ…å†µ
            if len(band_mi_matrices) == 2:
                bands = list(band_mi_matrices.keys())
                avg_mi_matrix = (band_mi_matrices[bands[0]] + band_mi_matrices[bands[1]]) / 2
            else:
                avg_mi_matrix = list(band_mi_matrices.values())[0]
        else:
            # ä¼ ç»Ÿä¸‰é¢‘æ®µçš„æƒ…å†µ
            alpha_mi = band_mi_matrices.get('alpha', np.zeros_like(list(band_mi_matrices.values())[0]))
            beta_mi = band_mi_matrices.get('beta', np.zeros_like(list(band_mi_matrices.values())[0]))
            gamma_mi = band_mi_matrices.get('gamma', np.zeros_like(list(band_mi_matrices.values())[0]))
            avg_mi_matrix = (alpha_mi + beta_mi + gamma_mi) / 3
        
        # è®¡ç®—æ¯ä¸ªé€šé“çš„è¿æ¥å¼ºåº¦
        channel_connectivity = np.sum(avg_mi_matrix, axis=1)
        
        # æ’åºè·å–å‰4ä¸ªé€šé“
        top_channels_indices = np.argsort(channel_connectivity)[-4:][::-1]
        top_channels_scores = channel_connectivity[top_channels_indices]
        top_channels_names = [self.ch_names[i] for i in top_channels_indices]
        
        return avg_mi_matrix, top_channels_indices, top_channels_scores, top_channels_names
    
    def analyze_all_trials_ultra_fast(self):
        """è¶…å¿«é€Ÿåˆ†ææ‰€æœ‰trials"""
        if self.verbose:
            print("===== å¼€å§‹è¶…é«˜é€Ÿäº’ä¿¡æ¯åˆ†æ =====")
        
        if not hasattr(self, 'density_labels'):
            self.calculate_density_labels_fast()
        
        num_trials = self.trials.shape[0]
        results = []
        
        start_time = time.time()
        
        for trial_idx in range(num_trials):
            # æ™ºèƒ½è¿›åº¦æ˜¾ç¤º
            if self.verbose:
                if trial_idx == 0:
                    print("å¼€å§‹å¤„ç†ç¬¬ä¸€ä¸ªtrial...")
                elif trial_idx % max(1, num_trials // 20) == 0:  # æ˜¾ç¤º20æ¬¡è¿›åº¦
                    elapsed = time.time() - start_time
                    rate = trial_idx / elapsed if elapsed > 0 else 0
                    remaining = (num_trials - trial_idx) / rate if rate > 0 else 0
                    print(f"è¿›åº¦: {trial_idx}/{num_trials} ({100*trial_idx/num_trials:.0f}%) - "
                          f"é€Ÿåº¦: {rate:.1f} trials/ç§’, é¢„è®¡å‰©ä½™: {remaining:.0f}ç§’")
            
            # åˆ†æå½“å‰trial
            band_mi_matrices = self.analyze_single_trial_ultra_fast(trial_idx)
            avg_mi_matrix, top_indices, top_scores, top_names = self.calculate_average_mi_and_rank_channels(band_mi_matrices)
            
            # è·å–trialä¿¡æ¯
            original_label = self.trial_labels[trial_idx]
            density_label = self.density_labels[trial_idx]
            cluster_label = self.cluster_labels[trial_idx]
            
            # ä¿å­˜ç»“æœ
            trial_result = {
                'trial_idx': trial_idx,
                'original_label': original_label,
                'density_label': density_label,
                'cluster_label': cluster_label,
                'top_4_channels': top_names,
                'top_4_indices': top_indices,
                'top_4_scores': top_scores,
            }
            
            results.append(trial_result)
        
        total_time = time.time() - start_time
        if self.verbose:
            print(f"\nğŸ‰ è¶…é«˜é€Ÿåˆ†æå®Œæˆ!")
            print(f"æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
            print(f"å¹³å‡é€Ÿåº¦: {num_trials/total_time:.1f} trials/ç§’")
            print(f"æ¯trialå¹³å‡: {total_time/num_trials:.2f}ç§’")
        
        self.results = results
        return results
    
    def save_results_to_csv(self, output_path='ultra_fast_mi_results.csv'):
        """ä¿å­˜ç»“æœåˆ°CSV"""
        if self.verbose:
            print("===== ä¿å­˜ç»“æœ =====")
        
        data_rows = []
        for result in self.results:
            row = {
                'trial_idx': result['trial_idx'],
                'original_label': result['original_label'],
                'density_label': result['density_label'],
                'cluster_label': result['cluster_label'],
                'top_channel_1': result['top_4_channels'][0],
                'top_channel_1_score': result['top_4_scores'][0],
                'top_channel_2': result['top_4_channels'][1],
                'top_channel_2_score': result['top_4_scores'][1],
                'top_channel_3': result['top_4_channels'][2],
                'top_channel_3_score': result['top_4_scores'][2],
                'top_channel_4': result['top_4_channels'][3],
                'top_channel_4_score': result['top_4_scores'][3],
            }
            data_rows.append(row)
        
        df = pd.DataFrame(data_rows)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        if self.verbose:
            print(f"ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        
        return df
    
    def run_ultra_fast_analysis(self, max_trials=None):
        """è¿è¡Œè¶…é«˜é€Ÿåˆ†ææµç¨‹"""
        if self.verbose:
            print("===== è¶…é«˜é€ŸEEGäº’ä¿¡æ¯åˆ†æ =====")
            print("âš¡ æè‡´ä¼˜åŒ–ç­–ç•¥:")
            print("  - ç®€åŒ–æ»¤æ³¢ç®—æ³•")
            print("  - å¤§å¹…æ•°æ®é‡‡æ ·")
            print("  - å‘é‡åŒ–è®¡ç®—")
            print("  - å‡å°‘é¢‘æ®µæ•°é‡")
        
        # 1. åŠ è½½æ•°æ®
        self.load_and_preprocess_data()
        
        # 2. å¿«é€Ÿè®¡ç®—å¯†åº¦æ ‡ç­¾
        self.calculate_density_labels_fast()
        
        # 3. æˆªå–æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if max_trials is not None and max_trials < self.trials.shape[0]:
            if self.verbose:
                print(f"æµ‹è¯•æ¨¡å¼ï¼šåªåˆ†æå‰ {max_trials} ä¸ªtrials")
            self.trials = self.trials[:max_trials]
            self.trial_labels = self.trial_labels[:max_trials]
            self.density_labels = self.density_labels[:max_trials]
            self.cluster_labels = self.cluster_labels[:max_trials]
        
        # 4. è¶…å¿«é€Ÿåˆ†æ
        self.analyze_all_trials_ultra_fast()
        
        # 5. ä¿å­˜ç»“æœ
        suffix = f'_first{max_trials}' if max_trials is not None else ''
        output_csv = f'ultra_fast_mi_results_{self.subject_id}{suffix}.csv'
        df = self.save_results_to_csv(output_csv)
        
        return self.results, df

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    quick_test = '--quick' in sys.argv or '-q' in sys.argv
    silent_mode = '--silent' in sys.argv or '-s' in sys.argv
    precision_mode = '--precision' in sys.argv
    
    max_trials = 5 if quick_test else None
    
    # æ£€æŸ¥è‡ªå®šä¹‰trialsæ•°é‡
    for arg in sys.argv:
        if arg.startswith('--trials='):
            try:
                max_trials = int(arg.split('=')[1])
                quick_test = True
            except ValueError:
                if not silent_mode:
                    print("âš ï¸ æ— æ•ˆçš„trialså‚æ•°")
    
    # åˆ›å»ºè¶…é«˜é€Ÿåˆ†æå™¨
    analyzer = UltraFastMutualInformationAnalyzer(
        subject_id='aw',
        random_state=42,
        verbose=not silent_mode,
        use_simple_filter=not precision_mode,  # ç²¾åº¦æ¨¡å¼ä½¿ç”¨æ›´å¥½çš„æ»¤æ³¢
        max_samples_per_trial=100 if not precision_mode else 500  # ç²¾åº¦æ¨¡å¼ä½¿ç”¨æ›´å¤šé‡‡æ ·ç‚¹
    )
    
    # æ˜¾ç¤ºæ¨¡å¼ä¿¡æ¯
    if not silent_mode:
        print("ğŸš€ è¶…é«˜é€ŸEEGäº’ä¿¡æ¯åˆ†æå™¨")
        print("=" * 50)
        if quick_test:
            print(f"âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼š{max_trials} trials")
        else:
            print("âš¡ å®Œæ•´åˆ†ææ¨¡å¼")
        
        if precision_mode:
            print("ğŸ¯ ç²¾åº¦æ¨¡å¼ï¼šä½¿ç”¨æ›´å¥½çš„æ»¤æ³¢å’Œæ›´å¤šé‡‡æ ·ç‚¹")
        else:
            print("ğŸš€ é€Ÿåº¦æ¨¡å¼ï¼šæè‡´ä¼˜åŒ–çš„å¿«é€Ÿåˆ†æ")
        
        print("\nå‘½ä»¤è¡Œé€‰é¡¹:")
        print("  --quick: å¿«é€Ÿæµ‹è¯• (5 trials)")
        print("  --silent: é™é»˜æ¨¡å¼")
        print("  --precision: ç²¾åº¦æ¨¡å¼ï¼ˆç¨æ…¢ä½†æ›´å‡†ç¡®ï¼‰")
        print("  --trials=N: è‡ªå®šä¹‰trialsæ•°é‡")
        print()
    
    # è¿è¡Œåˆ†æ
    start_time = time.time()
    results, df = analyzer.run_ultra_fast_analysis(max_trials=max_trials)
    total_time = time.time() - start_time
    
    # æ˜¾ç¤ºç»“æœ
    if not silent_mode:
        print(f"\n{'='*50}")
        print(f"ğŸ‰ è¶…é«˜é€Ÿåˆ†æå®Œæˆ!")
        print(f"{'='*50}")
        print(f"æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"å¤„ç†é€Ÿåº¦: {len(results)/total_time:.1f} trials/ç§’")
        print(f"å¹³å‡æ¯trial: {total_time/len(results):.3f}ç§’")
        
        if len(results) < 280:
            estimated_280 = total_time * 280 / len(results)
            print(f"280ä¸ªtrialsé¢„è®¡ç”¨æ—¶: {estimated_280/60:.1f}åˆ†é’Ÿ")
        
        print(f"\n===== æ ·ä¾‹ç»“æœ =====")
        for i in range(min(3, len(results))):
            result = results[i]
            print(f"Trial {result['trial_idx']}:")
            print(f"  åŸå§‹æ ‡ç­¾: {result['original_label']}")
            print(f"  å¯†åº¦æ ‡ç­¾: {result['density_label']}")
            print(f"  å‰4é€šé“: {result['top_4_channels']}")
            print()
    else:
        print(f"å®Œæˆ: {len(results)} trials, {total_time:.1f}s, {len(results)/total_time:.1f} trials/s")

if __name__ == '__main__':
    main()